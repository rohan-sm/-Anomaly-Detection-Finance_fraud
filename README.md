# Financial Fraud / Anomaly Detection API

This project is a **fraud detection system** that identifies **anomalous financial transactions** using **unsupervised machine learning**, specifically **Isolation Forest**, and exposes predictions through a **FastAPI REST API**.

The focus of this project is on **real-world fraud behavior modeling**, clean backend design, and explainable predictions â€” not just model accuracy.

---

## ğŸ” Problem Statement

In real financial systems:
- Fraud is **rare and highly imbalanced**
- Labels are often **missing or delayed**
- Fraud patterns change over time

Because of this, treating fraud detection as a **supervised classification problem** is often unreliable.

â¡ï¸ This project approaches fraud as an **anomaly detection problem**, where transactions that deviate strongly from normal user behavior are flagged as potential fraud.

---

## ğŸ—ï¸ System Architecture



```
FRAUD-ANAMILY-DETECTION
â”œâ”€â”€ app/                        # API & Inference logic
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ models/                     # Saved models (.pkl, .keras) & artifacts
â”œâ”€â”€ notebooks/                  # Experiments & Analysis
â”œâ”€â”€ reports/                    # Generated Metrics
â”œâ”€â”€ src/                        # Core Logic & Pipelines
â”‚   â”œâ”€â”€ data_generation/
â”‚   â”œâ”€â”€ feature_engineering/
â”‚   â”œâ”€â”€ models/                 # pipelines
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ tests/                      # Unit Tests
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_pipeline.py
```
---

## ğŸ¤– Models Used

### 1ï¸âƒ£ Isolation Forest

### 2ï¸âƒ£ One-Class SVM

### 3ï¸âƒ£ Autoencoder (Neural Network)


---

## ğŸ“Š Feature Engineering Philosophy

Fraud is rarely about raw values.
Itâ€™s about **contextual deviation**.

### Example Features
| Feature | Why It Matters |
|------|---------------|
| `amount_dev_log` | Detects abnormal spend magnitude |
| `avg_amount_24h` | Personal baseline modeling |
| `txn_count_1h` | Velocity attacks |
| `hour_sin / hour_cos` | Time-of-day anomalies |
| `location_change` | Geo inconsistency |
| `device_change` | Account takeover signal |

âš ï¸ **Same preprocessing pipeline is used during training and inference**  
(No trainingâ€“serving skew)
---
--------------------
## ğŸ—ï¸ High-Level Architecture
```
Transaction Input
      â”‚
      â–¼
FastAPI (/predict)
      â”‚
      â–¼
Feature Engineering
      â”‚
      â–¼
Isolation Forest Model
      â”‚
      â–¼
Anomaly Score
      â”‚
      â–¼
Fraud Probability + Reasoning
      â”‚
      â–¼
   Logging
```
## ğŸ§  Why Isolation Forest?

Multiple anomaly detection approaches were experimented with during development.

**Isolation Forest was chosen for the final system because:**
- It performed **most consistently** on validation data
- It scales well to large transaction volumes
- It works naturally with **tabular financial features**
- It provides a clear anomaly score that can be converted into a fraud probability

Rather than overengineering, the final system uses **one strong, explainable model**.

---


---

## ğŸ”® API Endpoint

### POST /predict

#### Request
```json
{
  "amount": 4500,
  "avg_amount_24h": 1200,
  "txn_count_1h": 6,
  "hour": 2,
  "location_change": 1,
  "device_change": 1
}
```
## Responce
```json
{
  "fraud_probability": 0.83,
  "is_fraud": true,
  "reasoning": [
    "Transaction amount deviates from user's normal spending",
    "Unusual transaction time detected",
    "High transaction velocity observed",
    "Isolation Forest anomaly score is high"
  ]
}
```

---
## ğŸ§® Fraud Probability Logic

Isolation Forest outputs an anomaly score, not a probability.
To make the output interpretable:
* Raw anomaly scores are normalized
* Scores are mapped to a range between 0 and 1
* A configurable threshold is used to classify fraud vs non-fraud
This keeps the decision logic:
* Simple
* Transparent
* Easy to explain in interviews

---
## ğŸªµ Logging

Every prediction is logged for traceability and debugging.
Each log entry includes:
* Timestamp
* Key input attributes
* Anomaly score
* Fraud probability
* Final fraud decision

---
## Validation & Error Handling

* Request validation using Pydantic
* Clear error messages for invalid inputs
* Safe handling of model loading and inference errors

---
## â–¶ï¸ Running the Application
```
pip install -r requirements.txt
uvicorn app.main:app --reload
```
---
## API documentation is available at:

```http://127.0.0.1:8000/docs```
